{
  "core_deconstruction": {
      "motivation": "å¼€æºæ¨¡å‹ä¸é—­æºæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®è·æ­£åœ¨æ‰©å¤§ã€‚å¼€æºæ¨¡å‹å­˜åœ¨ä¸‰ä¸ªå…³é”®ç¼ºé™·ï¼š1) æ¶æ„ä¸Šå¯¹æ™®é€šæ³¨æ„åŠ›æœºåˆ¶çš„ä¾èµ–å¯¼è‡´é•¿åºåˆ—æ•ˆç‡ä½ä¸‹ï¼›2) åè®­ç»ƒé˜¶æ®µè®¡ç®—èµ„æºæŠ•å…¥ä¸è¶³ï¼›3) åœ¨AIæ™ºèƒ½ä½“åœºæ™¯ä¸­ï¼Œæ³›åŒ–èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›æ˜æ˜¾è½åäºé—­æºæ¨¡å‹ã€‚",
      "method": "1. å¼•å…¥DeepSeekç¨€ç–æ³¨æ„åŠ›ï¼ˆDSAï¼‰ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒé•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚2. å¼€å‘ç¨³å®šã€å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®®ï¼Œå…è®¸åœ¨åè®­ç»ƒé˜¶æ®µè¿›è¡Œå¤§è§„æ¨¡è®¡ç®—æ‰©å±•ï¼ˆè®¡ç®—é¢„ç®—è¶…è¿‡é¢„è®­ç»ƒæˆæœ¬çš„10%ï¼‰ã€‚3. æå‡ºä¸€ä¸ªæ–°é¢–çš„æµç¨‹ï¼Œé€šè¿‡åœ¨å·¥å…·ä½¿ç”¨åœºæ™¯ä¸­å¤§è§„æ¨¡åˆæˆä»»åŠ¡æ•°æ®ï¼ˆè¶…è¿‡1800ä¸ªä¸åŒç¯å¢ƒå’Œ85000ä¸ªå¤æ‚æç¤ºï¼‰ï¼Œå°†æ¨ç†èƒ½åŠ›é›†æˆåˆ°æ™ºèƒ½ä½“ä»»åŠ¡ä¸­ã€‚",
      "result": "DeepSeek-V3.2åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¸Kimi-k2-thinkingå’ŒGPT-5æ€§èƒ½ç›¸ä¼¼ã€‚å…¶æ™ºèƒ½ä½“èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œåœ¨EvalSysç­‰å¼•å…¥çš„é•¿å°¾æ™ºèƒ½ä½“ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºé«˜æˆæœ¬æ•ˆç›Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚å…¶é«˜è®¡ç®—å˜ä½“DeepSeek-V3.2-Specialeåœ¨æ¨ç†èƒ½åŠ›ä¸Šä¸Gemini-3.0-ProæŒå¹³ï¼Œå¹¶åœ¨2025å¹´å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰å’Œå›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIOIï¼‰ä¸­è·å¾—é‡‘ç‰Œè¡¨ç°ã€‚",
      "gap": "è®ºæ–‡æŒ‡å‡ºï¼Œå°½ç®¡å¼€æºç¤¾åŒºåœ¨è¿›æ­¥ï¼Œä½†é—­æºä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½è½¨è¿¹ä»¥æ›´é™¡çš„é€Ÿåº¦åŠ é€Ÿï¼Œå¯¼è‡´å¼€æºä¸é—­æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ä¼¼ä¹åœ¨æ‰©å¤§ï¼Œè€Œéæ”¶æ•›ã€‚"
  },
  "logical_anchors": [
      {
          "anchor_text": "Through our analysis, we identify three critical deficiencies that limit the capability of open-source models in complex tasks. First, architecturally, the predominant reliance on vanilla attention (Vaswani et al., 2017) mechanisms severely constrains efficiency for long sequences. This inefficiency poses a substantial obstacle to both scalable deployment and effective post-training. Second, regarding resource allocation, open-source models suffer from insufficient computational investment during the post-training phase, limiting their performance on hard tasks. Finally, in the context of AI agents, open-source models demonstrate a marked lag in generalization and instruction-following capabilities compared to their proprietary counterparts (EvalSys, 2025; Li et al., 2025; Luo et al., 2025), hindering their effectiveness in real deployment.",
          "page_hint": 2,
          "chacha_comment": "è¿™é‡Œæ˜¯è®ºæ–‡çš„ç«‹è®ºåŸºçŸ³ï¼Œæ¸…æ™°æŒ‡å‡ºäº†å¼€æºæ¨¡å‹çš„ä¸‰å¤§ç—›ç‚¹ã€‚ç†è§£è¿™ä¸‰ç‚¹ï¼Œå°±æŠ“ä½äº†å…¨æ–‡è¦è§£å†³çš„æ ¸å¿ƒçŸ›ç›¾ã€‚"
      },
      {
          "anchor_text": "Given the index scores {ğ¼ğ‘¡,ğ‘  } for each query token hğ‘¡ , our fine-grained token selection mechanism retrieves only the key-value entries {cğ‘  } corresponding to the top-k index scores. Then, the attention output uğ‘¡ is computed by applying the attention mechanism between the query token hğ‘¡ and the sparsely selected key-value entries {cğ‘  }: uğ‘¡ = Attn\u0000hğ‘¡ , \bcğ‘  ğ¼ğ‘¡,ğ‘  âˆˆ Top-k\u0000ğ¼ğ‘¡,:\u0001 \u0001.",
          "page_hint": 3,
          "chacha_comment": "è¿™æ˜¯DSAï¼ˆDeepSeekç¨€ç–æ³¨æ„åŠ›ï¼‰çš„æ ¸å¿ƒæ“ä½œé€»è¾‘ã€‚å®ƒæ­ç¤ºäº†å¦‚ä½•ä»O(nÂ²)é™åˆ°O(nk)çš„å…³é”®ä¸€æ­¥ï¼šåªå…³æ³¨æœ€é‡è¦çš„top-kä¸ªtokenã€‚"
      },
      {
          "anchor_text": "As a direct result of this adjustment, the gradient of this KL estimator becomes unbiased, which eliminates systematic estimation errors, thereby facilitating stable convergence. This contrasts sharply with the original K3 estimator, particularly when the sampled tokens have substantially lower probabilities under the current policy than the reference policy, i.e., ğœ‹ğœƒ â‰ª ğœ‹ref. In such cases, the gradient of the K3 estimator assigns disproportionately large, unbounded weights to maximize the likelihood of these tokens, resulting in noisy gradient updates that accumulate to degrade sample quality in subsequent iterations and lead to unstable training dynamics.",
          "page_hint": 7,
          "chacha_comment": "è¿™é‡Œæ·±å…¥åˆ°äº†å¼ºåŒ–å­¦ä¹ ç¨³å®šæ€§çš„æŠ€æœ¯ç¡¬æ ¸ã€‚è§£é‡Šäº†ä¸ºä»€ä¹ˆä¼ ç»Ÿçš„K3ä¼°è®¡å™¨åœ¨ç­–ç•¥å·®å¼‚å¤§æ—¶ä¼šå‡ºé—®é¢˜ï¼Œä»¥åŠä»–ä»¬æå‡ºçš„æ— åKLä¼°è®¡å¦‚ä½•è§£å†³è¿™ä¸ªç—›ç‚¹ã€‚è¿™æ˜¯å®ç°å¤§è§„æ¨¡RLæ‰©å±•çš„å…³é”®æŠ€æœ¯ç»†èŠ‚ã€‚"
      },
      {
          "anchor_text": "We observed that replicating DeepSeek-R1â€™s strategyâ€”discarding reasoning content upon the arrival of the second round of messagesâ€”results in significant token inefficiency. This approach forces the model to redundantly re-reason through the entire problem for each subsequent tool call. To mitigate this, we developed a context management strictly tailored for tool-calling scenarios... Historical reasoning content is discarded only when a new user message is introduced to the conversation. If only tool-related messages (e.g., tool outputs) are appended, the reasoning content is retained throughout the interaction.",
          "page_hint": 9,
          "chacha_comment": "è¿™æ˜¯ä¸€ä¸ªéå¸¸å·§å¦™çš„å·¥ç¨‹æ´å¯Ÿã€‚å®ƒç‚¹å‡ºäº†åœ¨æ™ºèƒ½ä½“åœºæ™¯ä¸­ç®€å•å¥—ç”¨çº¯æ¨ç†ç­–ç•¥çš„å¼Šç«¯ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ€§çš„ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ï¼Œæ˜¯æå‡æ™ºèƒ½ä½“æ•ˆç‡çš„å…³é”®è®¾è®¡ã€‚"
      }
  ],
  "knowledge_discovery": [
      {
          "term": "DeepSeek Sparse Attention (DSA)",
          "definition": "ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡é—ªç”µç´¢å¼•å™¨å’Œç»†ç²’åº¦tokené€‰æ‹©æœºåˆ¶ï¼Œå°†æ ¸å¿ƒæ³¨æ„åŠ›å¤æ‚åº¦ä»O(LÂ²)é™ä½åˆ°O(Lk)ï¼Œå…¶ä¸­kæ˜¯é€‰æ‹©çš„tokenæ•°é‡ï¼ˆk << Lï¼‰ã€‚æ—¨åœ¨ä¿æŒé•¿ä¸Šä¸‹æ–‡æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—æˆæœ¬ã€‚"
      },
      {
          "term": "Group Relative Policy Optimization (GRPO)",
          "definition": "DeepSeek-V3.2ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®ƒåœ¨æ¯ç»„é‡‡æ ·å“åº”ä¸­ï¼Œé€šè¿‡ç»„å†…å¥–åŠ±å½’ä¸€åŒ–æ¥è®¡ç®—ä¼˜åŠ¿ä¼°è®¡ï¼Œå¹¶ç»“åˆKLæ•£åº¦æƒ©ç½šæ¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ã€‚"
      },
      {
          "term": "Specialist Distillation",
          "definition": "åè®­ç»ƒä¸­çš„ä¸€ä¸ªæ­¥éª¤ã€‚é¦–å…ˆä¸ºç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€ç¼–ç¨‹ï¼‰è®­ç»ƒä¸“é—¨çš„æ¨¡å‹ï¼Œç„¶åä½¿ç”¨è¿™äº›ä¸“å®¶æ¨¡å‹ç”Ÿæˆé¢†åŸŸç‰¹å®šçš„æ•°æ®ï¼Œç”¨äºæœ€ç»ˆæ£€æŸ¥ç‚¹çš„è®­ç»ƒï¼Œä»¥æå‡å„é¢†åŸŸæ€§èƒ½ã€‚"
      },
      {
          "term": "Lightning Indexer",
          "definition": "DSAçš„ä¸€ä¸ªç»„ä»¶ã€‚å®ƒè®¡ç®—æŸ¥è¯¢tokenå’Œå‰é¢tokenä¹‹é—´çš„ç´¢å¼•åˆ†æ•°ï¼Œä»¥å†³å®šå“ªäº›tokenè¢«é€‰æ‹©ã€‚å…¶å¤´æ•°å°‘ï¼Œå¯ç”¨FP8å®ç°ï¼Œè®¡ç®—æ•ˆç‡é«˜ã€‚"
      },
      {
          "term": "Off-Policy Sequence Masking",
          "definition": "åœ¨GRPOè®­ç»ƒä¸­ç”¨äºç¨³å®šè®­ç»ƒçš„ç­–ç•¥ã€‚å®ƒä¼šæ©ç é‚£äº›å…·æœ‰è´Ÿä¼˜åŠ¿ä¸”ç­–ç•¥åˆ†æ­§ï¼ˆæ—§ç­–ç•¥ä¸å½“å‰ç­–ç•¥çš„KLæ•£åº¦ï¼‰è¶…è¿‡é˜ˆå€¼çš„åºåˆ—ï¼Œä»¥é˜²æ­¢æœ‰å®³çš„ç¦»ç­–ç•¥æ ·æœ¬ç ´åä¼˜åŒ–è¿‡ç¨‹ã€‚"
      }
  ]
}